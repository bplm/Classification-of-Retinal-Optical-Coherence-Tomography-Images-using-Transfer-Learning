{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#-----------------------------------------------------------------------------\n#  Work for EfficientNetB3 Version1\n#  author @HarshitaSharma \n#  Code Collaborator : @Kottana Priyanka\n#  Dated : 11 December , 2020\n# Kaggle dataset used .\n#-----------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Classification of OPTICAL COHERENCE TOMOGRAPHY SCANS USING TRANSFER LEARNING APPROACH(FEATURE EXTRACTION + FINE TUNING)"},{"metadata":{},"cell_type":"markdown","source":"Optical Coherence Tomography (OCT) is a diagnostic imaging technique that uses light to obtain high resolution images of the cross-sectional structure of the retina. The images are used by ophthalmologists to diagnose and track age-related macular degeneration (AMD), diabetes and other systemic pathologies that manifest in the eyes. Compact, miniaturized OCT devices for remote and in-home monitoring are being developed, and there is a need to couple this with AI models and a deployment strategy that will work with such devices.\n\nWith this is mind we consider the newer state-of-the art EfficientNet family of models to classify OCT images of the retina.  The trade-off between accuracy and model size is well known and deep learning practitioners chose either a deeper, wider or higher input resolution CNN model to improve accuracy. With EfficientNet the concept of *compound scaling* was introduced. This scales (up or down) the depth, width and resolution in a principled manner so that models have a more optimal accuracy vs size/efficiency profile.  \n\nTo train the OCT classification model, we will use a transfer learning workflow and leverage pretrained weights from ImageNet. The first step is feature extraction where the base model weights are frozen. OCT retinal images are very different from the natural images of imagenet  so the next step is to fine-tune the model by unfreezing some fraction of the upper layers of the base model and retrain the model.\n"},{"metadata":{},"cell_type":"markdown","source":"## I. Setup"},{"metadata":{},"cell_type":"markdown","source":"### Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install -q efficientnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport glob\nfrom itertools import repeat\nfrom itertools import product\n\nimport efficientnet.tfkeras as efn\n\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom PIL import Image\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"The TensorFlow version is \", tf.__version__)\nprint(\"The Keras version is \", tf.keras.__version__)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Globals"},{"metadata":{},"cell_type":"markdown","source":"Note: Due to Kaggle runtime restrictions we have set up a flag to either train for a short or longer time.  \nUse the flag TRAIN_FT_2 to fine-tune for 5 or 10 epochs.    \nExample usage:\nTRAIN_FT_2 = 1 trains the model for 10 epochs. The learning rate schedule is reset at 6 epochs."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data path\nTRAIN_PATH = r'../input/kermany2018/OCT2017 /train/'\nVAL_PATH = r'../input/kermany2018/OCT2017 /val/'\nTEST_PATH = r'../input/kermany2018/OCT2017 /test/'\n\n# Model/path\nMODEL_WEIGHTS_PATH = r'../input/oct-efficientnetb3-feature-extraction/'\nFE_FILE = 'oct_efficientnetb3_fe_model.h5'\nFT_FILE = 'oct_efficientnetb3_ft_model.h5'\n\n\n# Parameters\nCLASSES = ['CNV', 'DME', 'DRUSEN', 'NORMAL']\nNUM_CLASSES = len(CLASSES)\nIMG_DIMS = 300  #224\nIMG_SHAPE = (IMG_DIMS, IMG_DIMS, 3)\n\nFE_EPOCHS = 7    # feature extraction epochs\nFT_EPOCHS = 5    # fine tuning epochs\n\nBATCH_SIZE = 16\nOPTIMIZER = Adam()\n\n# Training flags \n# due to Kaggle training time limits, we set a flag to fine-tune\n# for 5 (FT_1) or 5+5 (FT_2) epochs\nTRAIN_FT_2 = 0 # set to 0/1 to fine tune for 5 or 5+5 epoch","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## II. Data  \nEDA and image preprocessing utility functions"},{"metadata":{},"cell_type":"markdown","source":"### Load data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data, plot class distribution\n\ndef load_data(path, label, class_type):\n    \n    image_list = glob.glob(path + class_type + '/*')\n    label_list = list(repeat(label, len(image_list)))\n    \n    data = list(zip(image_list,label_list))\n    df = pd.DataFrame(data, columns = ['images', 'labels'])\n\n    return df\n        \n\ndef plot_dataset_imbalance(sample_distribution): \n    colors = ['blue', 'red', 'pink', 'turquoise']\n    fig = plt.figure()\n    ax = fig.add_axes([0,0,1,1])\n    ax.bar(CLASSES, sample_distribution, color=colors)\n    plt.show()\n        \n        \n        \nfor dataset, path in list(zip(['train', 'val', 'test'], [TRAIN_PATH, VAL_PATH, TEST_PATH])):\n  \n    for label,class_type in enumerate(CLASSES):\n        df_var = \"df_\" + dataset + \"_\" + class_type\n        vars()[df_var] = load_data(path, str(label), class_type)\n        sample_size_var = \"num_\" + class_type\n        vars()[sample_size_var] = vars()[df_var].shape[0]        \n        print('The size of ', df_var, ' is {} '.format(vars()[df_var].shape))\n    \n    sample_distribution = [num_CNV, num_DME, num_DRUSEN, num_NORMAL]\n    plot_dataset_imbalance(sample_distribution)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Display raw images for four classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot randomly selected images of each class\n\nfig=plt.figure(figsize=(12, 10))\ncols = 3\nrows = 4\n\nfor idx, rand_idx in enumerate(np.random.randint(0,1000, size=cols*rows)):\n    class_type = CLASSES[int(idx/cols)]\n    df_var = \"df_train_\" + class_type\n    img = vars()[df_var]['images'][rand_idx]\n    img = plt.imread(img)     \n    ax = fig.add_subplot(rows, cols, idx+1)\n    if idx%cols==0:\n        plt.ylabel(CLASSES[int(idx/cols)], fontsize=16)\n    plt.imshow(img, cmap='gist_gray')\n  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Upsampling imbalanced data, make train/test dataframes "},{"metadata":{},"cell_type":"markdown","source":"As clear from the histograms, 'CNV' class which is roughly 3x the 'DMV' and 4x the 'DRUSEN' class, dominates the training data set distribution. To rebalance the data set two  approaches are available:   \ni) use Random Oversampler/Undersampler (implemented in sklearn)\nii) SMOTE (implemented in sklearn)    \nThese methods are memory intensive and only feasible for very small data sets. \n\nWe take a different approach to oversampling minority class image data: we **add (concatenate) the minority class dataframes multiple times to the training dataset**. We then shuffle the joint dataframe to create a new oversampled dataset. This method is easy to implement for larger image data sets.\n  \nHere we  **oversample DME and DRUSEN by 2x each, partially balancing the dataset**. We do not balance the dataset completely because oversampling can introduce noise (overfitting) to the model. \nOnce the dataset is partially rebalanced, we compute class weights that are applied to the loss function so that the minority classes are weighted higher."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_set(rebalance=0):\n    \n    if rebalance == 0:\n        df_train_upsample = pd.concat([df_train_CNV, \n                               df_train_DME,\n                               df_train_DRUSEN,\n                               df_train_NORMAL], axis=0).reset_index(drop=True)\n    else:\n        df_train_upsample = pd.concat([df_train_CNV, \n                               #df_train_DME,\n                               #df_train_DRUSEN,\n                               df_train_DME, df_train_DME, \n                               df_train_DRUSEN, df_train_DRUSEN, \n                               df_train_NORMAL], axis=0).reset_index(drop=True)\n\n    df_train = shuffle(df_train_upsample)\n    print('The size of df_train is {}'.format(df_train.shape))\n      \n    return df_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = get_train_set(rebalance=0)\n#df_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The validation dataset has 30 samples which is very small. We combine the test and validation data to create the unseen test data. This test set has 1000 samples."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_set():\n    \n    df_test_combined = pd.concat([df_test_CNV, df_val_CNV, \n                                  df_test_DME, df_val_DME, \n                                  df_test_DRUSEN, df_val_DRUSEN,\n                                  df_test_NORMAL, df_val_NORMAL], \n                                 axis=0).reset_index(drop=True)\n    df_test = shuffle(df_test_combined)\n    print('The size of df_test is {}'.format(df_test.shape))\n\n    return df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_test = get_test_set()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The function below is supplied to create a validation split from the train set. We will however, not be training with a validation set so we will not be using this code."},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_val_split_data(df_train, test_size=0.05):\n# split the train set into a train/val set\n\n    # select the column that we will use for stratification\n    y = df_train['labels']\n    df_train, df_val = train_test_split(df_train, test_size=test_size, random_state=2020, stratify=y)\n\n    print('The size of df_train is {}'.format(df_train.shape))\n    print('The size of df_val is {}'.format(df_val.shape))\n    \n    return df_train, df_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_train, df_val = get_train_val_split_data(df_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot train/test set class distribution"},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_counts = df_train['labels'].value_counts().sort_index()\n#test_counts = df_test['labels'].value_counts().sort_index()\n\ndef plot_dataset_distribution(df_train, df_test):\n    \n    fig, ax = plt.subplots(1,2, figsize=(10, 6))\n    colors = ['blue', 'red', 'pink', 'turquoise']\n\n\n    for index,dataset in enumerate(['train', 'test']):\n    \n        df_var = \"df_\" + dataset \n        counts = vars()[df_var]['labels'].value_counts().sort_index()\n        ax[index].pie(counts, labels=CLASSES, autopct='%.1f %%', colors=colors)\n        ax[index].set_title( '{} set'.format(dataset))\n    \n    fig.suptitle('Class distribution for train and test datasets',\n                 y=1.1, fontsize=16)\n    fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot_dataset_distribution(df_train, df_test) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Get class weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_class_weights(verbose=1):\n    \n    counter = Counter(df_train.labels)                          \n    max_count = float(max(counter.values()))  \n    class_weights = {int(class_label) : max_count/num_images for class_label, \n                     num_images in counter.items()}                     \n    if verbose:\n        print('Class weights: \\n', class_weights)\n    return class_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class_weights = get_class_weights()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Find image size dimensions "},{"metadata":{"trusted":true},"cell_type":"code","source":"def find_img_dims(image_list):\n    \n    min_size = []\n    max_size = []\n    \n    for i in range(len(image_list)):\n        im = Image.open(image_list[i])\n        min_size.append(min(im.size))\n        max_size.append(max(im.size))\n    \n    return min(min_size), max(max_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_list = df_train.images\nmin_size, max_size = find_img_dims(image_list)\nprint('The min and max image dims are {} and {} respectively.'\n      .format(min_size, max_size))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have set the model input image size (IMG_DIMS) to 300x300 pixels which is the expected input resolution for the EfficientNet B3 model. This resolution is smaller than the smallest image in the data set."},{"metadata":{},"cell_type":"markdown","source":"### Image Augmentation/Data Generator"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_data(augmentation=0):\n    \n    if augmentation:\n        train_datagen = ImageDataGenerator(rescale=1./255,\n                                    rotation_range=20,\n                                    width_shift_range=0.2,\n                                    height_shift_range=0.2,\n                                    fill_mode='nearest',\n                                    zoom_range = 0.3,\n                                    horizontal_flip = True)        \n    else:    \n        train_datagen = ImageDataGenerator(rescale=1./255)\n        \n    \n    train_data= train_datagen.flow_from_dataframe(dataframe=df_train, \n                                            #directory=TRAIN_IMG_DIR, \n                                            directory=None,    # paths specified in x_col\n                                            x_col=\"images\", \n                                            y_col=\"labels\", \n                                            class_mode=\"categorical\",  # for multiclass\n                                            target_size=(IMG_DIMS, IMG_DIMS),\n                                            batch_size=BATCH_SIZE)\n    return train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = get_train_data(augmentation=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train_data)\ntrain_data=list(train_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_idx = np.random.randint(1, len(train_data), size=9)\nfig, axes = plt.subplots(4, 3, figsize=(12, 10))\n\nfor idx, ax in enumerate(axes.ravel()):\n    img = Image.open(train_data[idx])\n    ax.set_title(labels[idx])\n    ax.imshow(img)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for idx, rand_idx in train_data:\n    class_type = CLASSES[int(idx/cols)]\n    df_var = \"df_train_\" + class_type\n    img = vars()[df_var]['images'][rand_idx]\n    img = plt.imread(img)     \n    ax = fig.add_subplot(rows, cols, idx+1)\n    if idx%cols==0:\n        plt.ylabel(CLASSES[int(idx/cols)], fontsize=16)\n    plt.imshow(img, cmap='gist_gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_valid_data():\n    \n    valid_datagen = ImageDataGenerator(rescale=1./255)\n    valid_data = valid_datagen.flow_from_dataframe(dataframe=df_val, \n                                             directory=None, \n                                             x_col=\"images\", \n                                             y_col=\"labels\", \n                                             class_mode=\"categorical\",\n                                             shuffle= True,\n                                             target_size=(IMG_DIMS, IMG_DIMS),\n                                             batch_size=BATCH_SIZE)\n    return valid_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_data():\n    \n    test_datagen = ImageDataGenerator(rescale=1./255)\n    test_data = test_datagen.flow_from_dataframe(dataframe=df_test, \n                                             directory=None, \n                                             x_col=\"images\", \n                                             y_col=\"labels\", \n                                             class_mode=\"categorical\",\n                                             shuffle= False,\n                                             target_size=(IMG_DIMS, IMG_DIMS),\n                                             batch_size=BATCH_SIZE)\n    return test_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## III. Training  \nUtility functions (plotting, callbacks)"},{"metadata":{},"cell_type":"markdown","source":"### Plot training performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"def display_training_curves(accuracy, loss, start_epoch):\n       \n    fig, ax = plt.subplots(2,1, figsize=(10, 8))\n   \n\n    for index,lcurve in enumerate(['accuracy', 'loss']):\n    \n        df_var = lcurve \n        data = vars()[df_var]\n        ax[index].plot(data, color='b' if lcurve=='accuracy' else 'r')\n        ax[index].set_title( 'Training {}'.format(lcurve))\n        if start_epoch >= FE_EPOCHS: \n            ax[index].plot([FE_EPOCHS, FE_EPOCHS], \n                 plt.ylim(), linestyle='--', label='Fine tuning 1')\n        if start_epoch >= FE_EPOCHS+FT_EPOCHS: \n            ax[index].plot([FE_EPOCHS+FT_EPOCHS, FE_EPOCHS+FT_EPOCHS], \n                 plt.ylim(), linestyle='--', label='Fine Tuning 2')\n        ax[index].legend(loc='lower left')\n    \n    plt.xlabel('epochs')\n    fig.suptitle('Training curves',\n                 y=1.1, fontsize=16)\n    fig.tight_layout()\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"class LRFinder(Callback):\n    \n    '''\n    The Learning Rate range test: a callback for finding the optimal learning rate range  \n    This function will \n    \n    # Usage\n        ```\n            lr_finder = LRFinder(min_lr=1e-5, \n                                 max_lr=1e-2, \n                                 steps_per_epoch=np.ceil(data_size/batch_size),  \n                                 epochs=3\n                                 beta=0.9)\n            model.fit(X_train, Y_train, callbacks=[lr_finder])\n            \n            lr_finder.plot_loss()\n        ```\n    \n    # Arguments\n        min_lr: The lower bound of the learning rate  \n        max_lr: The upper bound of the learning rate \n        steps_per_epoch: Number of iterations/mini-batches -- calculated as `np.ceil(data_size/batch_size)`. \n        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n        beta: the smoothing parameter. 0.99 ~ weighted over 100 previous values, \n                                       0.9 - 10 values.\n        \n    # Acknowledgements\n        https://raw.githubusercontent.com/Meena-Mani/IDC_breast_cancer/master/lrate_callback.py\n        Original paper: https://arxiv.org/abs/1506.01186\n\n    '''\n    \n    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None, beta=0.9):\n        super().__init__()\n        \n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.total_iterations = steps_per_epoch * epochs\n        self.iteration = 0\n        self.history = {}\n        self.beta = beta\n        \n    def clr(self):\n        '''Calculate the learning rate.'''\n        #use log fn to sample very small values\n        x = np.log(1 + self.iteration / self.total_iterations) #use log fn to sample very small values\n        return self.min_lr + (self.max_lr-self.min_lr) * x\n        \n    def on_train_begin(self, logs=None):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n        tf.keras.backend.set_value(self.model.optimizer.lr, self.min_lr)\n        \n    def on_batch_end(self, epoch, logs=None):\n        '''For every iteration, record batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.iteration += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.iteration)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n            \n        tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())\n \n\n    def smooth_fn(self, y):\n        '''Helper function to smooth input over a weighted average.'''\n        n = len(self.history['iterations'])\n        beta_c = 1 - self.beta\n        ewa = np.zeros(n)\n        ewa_corrected = np.zeros(n)\n        ewa_corrected[0] = ewa[0] = y[0]\n        for i in range (1,n):\n            ewa[i] = self.beta*ewa[i-1] + beta_c*y[i] \n            ewa_corrected[i] = ewa[i] / (1 - self.beta**n)\n        return ewa_corrected\n\n    def plot_lr(self):\n        '''Helper function to quickly inspect the learning rate schedule.'''\n        plt.figure(figsize=(10,6))\n        plt.plot(self.history['iterations'], self.history['lr'])\n        plt.yscale('log')\n        plt.xlabel('Iteration')\n        plt.ylabel('LR')\n        plt.title(\"Learning rate\")\n        \n    def plot_loss(self):\n        '''Plot the loss versus the learning rate'''\n        plt.figure(figsize=(10,6))\n        smoothed_loss = self.smooth_fn(self.history['loss'])\n        plt.plot(self.history['lr'][1::10], smoothed_loss[1::10])\n        plt.xscale('log')\n        plt.xlabel('LR (log scale)')\n        plt.ylabel('Loss')\n        plt.title(\"Loss vs Learning Rate\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Callbacks\n\n\ndef lrfn(epoch, lr):\n    \n    if epoch == 1:\n        lr = lr\n    elif epoch<=3:\n        lr = lr**1.1\n    else :\n        lr = lr / 2\n\n    return lr\n\n\nlr_scheduler = LearningRateScheduler(lrfn,verbose=1) \n\n\ncheckpoint = ModelCheckpoint(filepath='/kaggle/working/best_weights.hdf5', \n                             save_best_only=True, save_weights_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transfer Learning step 1: Feature Extraction "},{"metadata":{},"cell_type":"markdown","source":"EfficientNet considerations:  \nWe use the EfficientNet B3 model since it is a mobile-sized architecture with 11M trainable parameters with a higher accuracy than models in this class.\n\n\nSome strategies used for feature extraction:  \n- small batch size for faster convergence to a good solution (batch size = 16)\n- larger learning rate (we use the default Adam lr: lr = 0.001)\n- no data augmentation\n- do not rebalance the data with oversampling; use class weights "},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = get_train_set(rebalance=0)\n\ntrain_data = get_train_data(augmentation=0)\n\nclass_weights = get_class_weights()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = get_test_set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dataset_distribution(df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the base model from the pre-trained model EfficientNet B3\n\nbase_model = efn.EfficientNetB3(input_shape=IMG_SHAPE,\n                                include_top=False, \n                                weights='imagenet',\n                               )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model.trainable = False\n#base_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# build model\n\nmodel = tf.keras.Sequential([\n  base_model,\n  layers.GlobalAveragePooling2D(),\n  layers.Dense(NUM_CLASSES, activation='softmax'),\n])\n\n\nmodel.summary()\nplot_model(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Find learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to find/plot the learning rate using the LRFinder callback\n\ndef find_lr(MIN_LR=1e-5, MAX_LR=1e-2, STEPS=train_data.samples // BATCH_SIZE, \n            EPOCHS=3, CLASS_WEIGHTS={0: 1.0, 1: 3.3, 2: 4.3, 3: 1.4}, \n            beta = 0.99):\n    \n    lr_finder = LRFinder(min_lr=MIN_LR, \n                         max_lr=MAX_LR, \n                         steps_per_epoch=STEPS, \n                         epochs=EPOCHS,\n                         beta = beta)\n    \n    history = model.fit(\n        train_data,\n        steps_per_epoch=train_data.samples // BATCH_SIZE,\n        epochs=3,\n        #workers=3,\n        callbacks=[lr_finder],\n        class_weight=CLASS_WEIGHTS)\n    \n    lr_finder.plot_loss()\n    lr_finder.plot_lr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OPTIMIZER = Adam()\nmodel.compile(optimizer=OPTIMIZER, \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])\n\nfind_lr(CLASS_WEIGHTS=class_weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A learning rate of 0.001 or 0.002 will give the best performance. The default for the Adam optimizer is 0.001 so we will use that value. This choice is supported by experiments we did using LR = 0.001, 0.002 and 0.01. As indicated by the LR curve above, 0.01 is probably too large and the model had problems  converging. 0.002 was also suboptimal."},{"metadata":{},"cell_type":"markdown","source":"#### Feature Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"OPTIMIZER = Adam(lr=0.001)\nmodel.compile(optimizer=OPTIMIZER, \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_FE = model.fit(\n           train_data, \n           steps_per_epoch= train_data.samples // BATCH_SIZE, \n           epochs=FE_EPOCHS,\n           class_weight=class_weights, \n           #workers=4,\n           #validation_data=valid_data, \n           #validation_steps=valid_data.samples // BATCH_SIZE,\n           callbacks=[checkpoint])\n\nmodel.save_weights(FE_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot training curves\n    \naccuracy = history_FE.history['accuracy']\nloss = history_FE.history['loss']\ndisplay_training_curves(accuracy, loss, start_epoch=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transfer Learning step 2: Fine-tuning"},{"metadata":{},"cell_type":"markdown","source":"This is the second stage of transfer learning.  We unfreeze some fraction of the layers and fit the model using smaller learning rate (Adam lr = 0.0001). \n\nWe address the  data imbalance by:\n- rebalancing the training dataset (oversampling with copies of DME and DRUSEN)\n- augmenting the data \n- using class weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = get_train_set(rebalance=1)\ntrain_data = get_train_data(augmentation=1)\n    \nclass_weights = get_class_weights()\n    \nplot_dataset_distribution(df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"\nThe training dataset is now more balanced (but not completely equal)."},{"metadata":{},"cell_type":"markdown","source":"# **The FINE TUNING APPROACH!!!**\n**WE KEEP THE BATCH NORMALIZATION LAYERS IN INFERENCE MODE TO SEE THE RESULTS PROFILE AND ITS EFFECT**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# set layers from 'block7a_se_excite' trainable\n    \nprint(\"Number of layers in the base model: \", len(base_model.layers))\n# plot_model(base_model)\n\n\nbase_model.trainable = True\n\nset_trainable = False\nfor layer in base_model.layers:\n    if layer.name == 'block7a_se_excite':\n        set_trainable = True\n    if set_trainable:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n    else:\n        layer.trainable = False\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After unfreezing **block7a_se_excite** and its successive layers (this is roughly the top 10% of the layers), the number of trainable parameters is now 3.38M."},{"metadata":{},"cell_type":"markdown","source":"#### Train for the first 5 epochs (FT_1)"},{"metadata":{"trusted":true},"cell_type":"code","source":"OPTIMIZER = Adam(lr=1e-4)\nmodel.compile(optimizer=OPTIMIZER, \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_FT_1 = model.fit(\n            train_data, \n            steps_per_epoch = train_data.samples // BATCH_SIZE, \n            epochs = FT_EPOCHS,\n            class_weight = class_weights, \n            #workers = 4,\n            #validation_data=valid_data, \n            #validation_steps=valid_data.samples // BATCH_SIZE,\n            callbacks=[checkpoint, lr_scheduler])\n    \nmodel.save_weights(FT_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot FE and FT learning curves \n\naccuracy += history_FT_1.history['accuracy']\nloss += history_FT_1.history['loss']\ndisplay_training_curves(accuracy, loss, start_epoch=FE_EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Train for another 5 epochs (FT_2)"},{"metadata":{"trusted":true},"cell_type":"code","source":"if TRAIN_FT_2:\n    \n    OPTIMIZER = Adam(lr=1e-4)\n    model.compile(optimizer=OPTIMIZER, \n                  loss='categorical_crossentropy', \n                  metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if TRAIN_FT_2:\n    \n    history_FT_2 = model.fit(\n                train_data, \n                steps_per_epoch = train_data.samples // BATCH_SIZE, \n                epochs = FT_EPOCHS,\n                class_weight = class_weights, \n                #workers = 4,\n                #validation_data=valid_data, \n                #validation_steps=valid_data.samples // BATCH_SIZE,\n                callbacks=[checkpoint, lr_scheduler])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot learning curves for FE, FT_1, FT_2\n\nif TRAIN_FT_2:\n    \n    accuracy += history_FT_2.history['accuracy']\n    loss += history_FT_2.history['loss']\n    display_training_curves(accuracy, loss, start_epoch=FE_EPOCHS+FT_EPOCHS)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## IV. Test Performance"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for test resultes only\n# model.load_weights(MODEL_WEIGHTS_PATH+FT_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, \n                          classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Purples):\n   \n\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy  \n    \n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    \n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, fontsize=15)\n    plt.yticks(tick_marks, classes, fontsize=15)\n    \n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'\n           .format(accuracy, misclass),fontsize=15)\n    plt.ylabel('True label', fontsize=15)\n    plt.title(title, fontsize=22);\n    plt.colorbar()\n\n    thresh = cm.max() / 2.0\n    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the model used for the test results \n\nif TRAIN_FT_2:\n    print(\"Test results for the fine tuning model trained for 5+5 epochs\")\nelse:\n    print(\"Test results for the fine tuning model trained for 5 epochs\")    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_test = get_test_set()\ntest_data = get_test_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prob = model.predict(test_data, steps=len(test_data), verbose=1)\ny_pred = np.argmax(y_prob,axis=1)\ny_true = df_test['labels'].astype('int64').to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print results\n\nprint(classification_report(y_true, y_pred))\nprint('The accuracy is {}'.format(accuracy_score(y_true, y_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\n    \ncm = confusion_matrix(y_true,y_pred)\nclasses = np.array(CLASSES)\ntitle = 'Confusion matrix of results'\nplot_confusion_matrix(cm, classes, title)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}