{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"#-----------------------------------------------------------------------------\n#  Work for MobileNet\n#  author @HarshitaSharma\n#  Code Collaborator : @Kottana Priyanka\n#  Dated : 11 December , 2020\n# Kaggle dataset used .\n#-----------------------------------------------------------------------------","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n  #  for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!pip install resnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras\nimport matplotlib.pyplot as plt\nfrom keras.applications.resnet50 import ResNet50, preprocess_input\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications import MobileNet\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import confusion_matrix, make_scorer, accuracy_score, classification_report\nimport seaborn as sns\n\n\n\nfrom keras.applications.inception_v3 import InceptionV3\n\n#from generator import DataGenerator\nfrom keras.callbacks import ModelCheckpoint\n\nimport tensorflow as tf\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport glob\nfrom itertools import repeat\nfrom itertools import product\n\n\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom PIL import Image\n\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom collections import Counter\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"The TensorFlow version is \", tf.__version__)\nprint(\"The Keras version is \", tf.keras.__version__)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data path\nTRAIN_PATH = r'../input/kermany2018/OCT2017 /train/'\nVAL_PATH = r'../input/kermany2018/OCT2017 /val/'\nTEST_PATH = r'../input/kermany2018/OCT2017 /test/'\n\n# Model/path\nMODEL_WEIGHTS_PATH = r'../input/oct-resnet-feature-extraction/'\nFE_FILE = 'oct_mobilenet_fe_model.h5'\nFT_FILE = 'oct_mobilenet_ft_model.h5'\n\n\n# Parameters\nCLASSES = ['CNV', 'DME', 'DRUSEN', 'NORMAL']\nNUM_CLASSES = len(CLASSES)\nIMG_DIMS = 300  #224\nIMG_SHAPE = (IMG_DIMS, IMG_DIMS, 3)\n\nFE_EPOCHS = 7   # feature extraction epochs\nFT_EPOCHS = 5   # fine tuning epochs (LR decr.)\n\nBATCH_SIZE = 16\nOPTIMIZER = Adam()\n\n# Training flags \n# due to Kaggle training time limits, we set a flag to fine-tune\n# for 5 (FT_1) or 5+5 (FT_2) epochs\nTRAIN_FT_2 = 0 # set to 0/1 to fine tune for 5 or 5+5 epoch\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# load data, plot class distribution\n\ndef load_data(path, label, class_type):\n    \n    image_list = glob.glob(path + class_type + '/*')\n    label_list = list(repeat(label, len(image_list)))\n    \n    data = list(zip(image_list,label_list))\n    df = pd.DataFrame(data, columns = ['images', 'labels'])\n\n    return df\n        \n\ndef plot_dataset_imbalance(sample_distribution): \n    colors = ['blue', 'red', 'pink', 'turquoise']\n    fig = plt.figure()\n    ax = fig.add_axes([0,0,1,1])\n    ax.bar(CLASSES, sample_distribution, color=colors)\n    plt.show()\n        \n        \n        \nfor dataset, path in list(zip(['train', 'val', 'test'], [TRAIN_PATH, VAL_PATH, TEST_PATH])):\n  \n    for label,class_type in enumerate(CLASSES):\n        df_var = \"df_\" + dataset + \"_\" + class_type\n        vars()[df_var] = load_data(path, str(label), class_type)\n        sample_size_var = \"num_\" + class_type\n        vars()[sample_size_var] = vars()[df_var].shape[0]        \n        print('The size of ', df_var, ' is {} '.format(vars()[df_var].shape))\n    \n    sample_distribution = [num_CNV, num_DME, num_DRUSEN, num_NORMAL]\n    plot_dataset_imbalance(sample_distribution)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfig=plt.figure(figsize=(12, 10))\ncols = 3\nrows = 4\n\nfor idx, rand_idx in enumerate(np.random.randint(0,1000, size=cols*rows)):\n    class_type = CLASSES[int(idx/cols)]\n    df_var = \"df_train_\" + class_type\n    img = vars()[df_var]['images'][rand_idx]\n    img = plt.imread(img)     \n    ax = fig.add_subplot(rows, cols, idx+1)\n    if idx%cols==0:\n        plt.ylabel(CLASSES[int(idx/cols)], fontsize=16)\n    plt.imshow(img, cmap='gist_gray')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_set(rebalance=0):\n    \n    if rebalance == 0:\n        df_train_upsample = pd.concat([df_train_CNV, \n                               df_train_DME,\n                               df_train_DRUSEN,\n                               df_train_NORMAL], axis=0).reset_index(drop=True)\n    else:\n        df_train_upsample = pd.concat([df_train_CNV, \n                               #df_train_DME,\n                               #df_train_DRUSEN,\n                               df_train_DME, df_train_DME, \n                               df_train_DRUSEN, df_train_DRUSEN, \n                               df_train_NORMAL], axis=0).reset_index(drop=True)\n\n    df_train = shuffle(df_train_upsample)\n    print('The size of df_train is {}'.format(df_train.shape))\n      \n    return df_train\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = get_train_set(rebalance=0)\ndf_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_set():\n    \n    df_test_combined = pd.concat([df_test_CNV, df_val_CNV, \n                                  df_test_DME, df_val_DME, \n                                  df_test_DRUSEN, df_val_DRUSEN,\n                                  df_test_NORMAL, df_val_NORMAL], \n                                 axis=0).reset_index(drop=True)\n    df_test = shuffle(df_test_combined)\n    print('The size of df_test is {}'.format(df_test.shape))\n\n    return df_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = get_test_set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_val_split_data(df_train, test_size=0.05):\n# split the train set into a train/val set\n\n    # select the column that we will use for stratification\n    y = df_train['labels']\n    df_train, df_val = train_test_split(df_train, test_size=test_size, random_state=2020, stratify=y)\n\n    print('The size of df_train is {}'.format(df_train.shape))\n    print('The size of df_val is {}'.format(df_val.shape))\n    \n    return df_train, df_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_dataset_distribution(df_train, df_test):\n    \n    fig, ax = plt.subplots(1,2, figsize=(10, 6))\n    colors = ['blue', 'red', 'pink', 'turquoise']\n\n\n    for index,dataset in enumerate(['train', 'test']):\n    \n        df_var = \"df_\" + dataset \n        counts = vars()[df_var]['labels'].value_counts().sort_index()\n        ax[index].pie(counts, labels=CLASSES, autopct='%.1f %%', colors=colors)\n        ax[index].set_title( '{} set'.format(dataset))\n    \n    fig.suptitle('Class distribution for train and test datasets',\n                 y=1.1, fontsize=16)\n    fig.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train, df_val = get_train_val_split_data(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_counts = df_train['labels'].value_counts().sort_index()\n#test_counts = df_test['labels'].value_counts().sort_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dataset_distribution(df_train, df_test) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get class weights\ndef get_class_weights(verbose=1):\n    \n    counter = Counter(df_train.labels)                          \n    max_count = float(max(counter.values()))  \n    class_weights = {int(class_label) : max_count/num_images for class_label, \n                     num_images in counter.items()}                     \n    if verbose:\n        print('Class weights: \\n', class_weights)\n    return class_weights","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weights = get_class_weights()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Find image size dimensions\ndef find_img_dims(image_list):\n    \n    min_size = []\n    max_size = []\n    \n    for i in range(len(image_list)):\n        x=image_list[i]\n        im = Image.open(x)\n        min_size.append(min(im.size))\n        max_size.append(max(im.size))\n    \n    return min(min_size), max(max_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_list = df_train.images\nimage_list=list(image_list)\nprint(len(image_list))\nmin_size, max_size = find_img_dims(image_list)\nprint('The min and max image dims are {} and {} respectively.'\n      .format(min_size, max_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We have set the model input image size (IMG_DIMS) to 300x300 pixels which is the expected input resolution for the EfficientNet B3 model. This resolution is smaller than the smallest image in the data set.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Image Augmentation/Data Generator\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_data(augmentation=0):\n    \n    if augmentation:\n        train_datagen = ImageDataGenerator(rescale=1./255,\n                                    rotation_range=20,\n                                    width_shift_range=0.2,\n                                    height_shift_range=0.2,\n                                    fill_mode='nearest',\n                                    zoom_range = 0.3,\n                                    horizontal_flip = True)        \n    else:    \n        train_datagen = ImageDataGenerator(rescale=1./255)\n        \n    \n    train_data= train_datagen.flow_from_dataframe(dataframe=df_train, \n                                            #directory=TRAIN_IMG_DIR, \n                                            directory=None,    # paths specified in x_col\n                                            x_col=\"images\", \n                                            y_col=\"labels\", \n                                            class_mode=\"categorical\",  # for multiclass\n                                            target_size=(IMG_DIMS, IMG_DIMS),\n                                            batch_size=BATCH_SIZE)\n    return train_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_valid_data():\n    \n    valid_datagen = ImageDataGenerator(rescale=1./255)\n    valid_data = valid_datagen.flow_from_dataframe(dataframe=df_val, \n                                             directory=None, \n                                             x_col=\"images\", \n                                             y_col=\"labels\", \n                                             class_mode=\"categorical\",\n                                             shuffle= True,\n                                             target_size=(IMG_DIMS, IMG_DIMS),\n                                             batch_size=BATCH_SIZE)\n    return valid_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_test_data():\n    \n    test_datagen = ImageDataGenerator(rescale=1./255)\n    test_data = test_datagen.flow_from_dataframe(dataframe=df_test, \n                                             directory=None, \n                                             x_col=\"images\", \n                                             y_col=\"labels\", \n                                             class_mode=\"categorical\",\n                                             shuffle= False,\n                                             target_size=(IMG_DIMS, IMG_DIMS),\n                                             batch_size=BATCH_SIZE)\n    return test_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Training\n#Plot training performance\ndef display_training_curves(accuracy, loss, start_epoch):\n       \n    fig, ax = plt.subplots(2,1, figsize=(10, 8))\n   \n\n    for index,lcurve in enumerate(['accuracy', 'loss']):\n    \n        df_var = lcurve \n        data = vars()[df_var]\n        ax[index].plot(data, color='b' if lcurve=='accuracy' else 'r')\n        ax[index].set_title( 'Training {}'.format(lcurve))\n        if start_epoch >= FE_EPOCHS: \n            ax[index].plot([FE_EPOCHS, FE_EPOCHS], \n                 plt.ylim(), linestyle='--', label='Fine tuning 1')\n        if start_epoch >= FE_EPOCHS+FT_EPOCHS: \n            ax[index].plot([FE_EPOCHS+FT_EPOCHS, FE_EPOCHS+FT_EPOCHS], \n                 plt.ylim(), linestyle='--', label='Fine Tuning 2')\n        ax[index].legend(loc='lower left')\n    \n    plt.xlabel('epochs')\n    fig.suptitle('Training curves',\n                 y=1.1, fontsize=16)\n    fig.tight_layout()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Callbacks\nclass LRFinder(Callback):\n    \n    '''\n    The Learning Rate range test: a callback for finding the optimal learning rate range  \n    This function will \n    \n    # Usage\n        ```\n            lr_finder = LRFinder(min_lr=1e-5, \n                                 max_lr=1e-2, \n                                 steps_per_epoch=np.ceil(data_size/batch_size),  \n                                 epochs=3\n                                 beta=0.9)\n            model.fit(X_train, Y_train, callbacks=[lr_finder])\n            \n            lr_finder.plot_loss()\n        ```\n    \n    # Arguments\n        min_lr: The lower bound of the learning rate  \n        max_lr: The upper bound of the learning rate \n        steps_per_epoch: Number of iterations/mini-batches -- calculated as `np.ceil(data_size/batch_size)`. \n        epochs: Number of epochs to run experiment. Usually between 2 and 4 epochs is sufficient. \n        beta: the smoothing parameter. 0.99 ~ weighted over 100 previous values, \n                                       0.9 - 10 values.\n        \n    # Acknowledgements\n        https://raw.githubusercontent.com/Meena-Mani/IDC_breast_cancer/master/lrate_callback.py\n        Original paper: https://arxiv.org/abs/1506.01186\n\n    '''\n    \n    def __init__(self, min_lr=1e-5, max_lr=1e-2, steps_per_epoch=None, epochs=None, beta=0.9):\n        super().__init__()\n        \n        self.min_lr = min_lr\n        self.max_lr = max_lr\n        self.total_iterations = steps_per_epoch * epochs\n        self.iteration = 0\n        self.history = {}\n        self.beta = beta\n        \n    def clr(self):\n        '''Calculate the learning rate.'''\n        #use log fn to sample very small values\n        x = np.log(1 + self.iteration / self.total_iterations) #use log fn to sample very small values\n        return self.min_lr + (self.max_lr-self.min_lr) * x\n        \n    def on_train_begin(self, logs=None):\n        '''Initialize the learning rate to the minimum value at the start of training.'''\n        logs = logs or {}\n        tf.keras.backend.set_value(self.model.optimizer.lr, self.min_lr)\n        \n    def on_batch_end(self, epoch, logs=None):\n        '''For every iteration, record batch statistics and update the learning rate.'''\n        logs = logs or {}\n        self.iteration += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.iteration)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n            \n        tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())\n \n\n    def smooth_fn(self, y):\n        '''Helper function to smooth input over a weighted average.'''\n        n = len(self.history['iterations'])\n        beta_c = 1 - self.beta\n        ewa = np.zeros(n)\n        ewa_corrected = np.zeros(n)\n        ewa_corrected[0] = ewa[0] = y[0]\n        for i in range (1,n):\n            ewa[i] = self.beta*ewa[i-1] + beta_c*y[i] \n            ewa_corrected[i] = ewa[i] / (1 - self.beta**n)\n        return ewa_corrected\n\n    def plot_lr(self):\n        '''Helper function to quickly inspect the learning rate schedule.'''\n        plt.figure(figsize=(10,6))\n        plt.plot(self.history['iterations'], self.history['lr'])\n        plt.yscale('log')\n        plt.xlabel('Iteration')\n        plt.ylabel('LR')\n        plt.title(\"Learning rate\")\n        \n    def plot_loss(self):\n        '''Plot the loss versus the learning rate'''\n        plt.figure(figsize=(10,6))\n        smoothed_loss = self.smooth_fn(self.history['loss'])\n        plt.plot(self.history['lr'][1::10], smoothed_loss[1::10])\n        plt.xscale('log')\n        plt.xlabel('LR (log scale)')\n        plt.ylabel('Loss')\n        plt.title(\"Loss vs Learning Rate\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Callbacks\n\n\ndef lrfn(epoch, lr):\n    \n    if epoch == 1:\n        lr = lr\n    elif epoch<=3:\n        lr = lr**1.1\n    else :\n        lr = lr / 2\n\n    return lr\n\n\nlr_scheduler = LearningRateScheduler(lrfn,verbose=1) \n\n\ncheckpoint = ModelCheckpoint(filepath='/kaggle/working/best_weights.hdf5', \n                             save_best_only=True, save_weights_only=True)\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transfer Learning step 1: Feature Extraction\nEfficientNet considerations:\nWe use the EfficientNet B3 model since it is a mobile-sized architecture with 11M trainable parameters with a higher accuracy than models in this class.\n\nSome strategies used for feature extraction:\n\nsmall batch size for faster convergence to a good solution (batch size = 16)\nlarger learning rate (we use the default Adam lr: lr = 0.001)\nno data augmentation\ndo not rebalance the data with oversampling; use class weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = get_train_set(rebalance=0)\n\ntrain_data = get_train_data(augmentation=0)\n\nclass_weights = get_class_weights()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_test = get_test_set()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_dataset_distribution(df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ****The model!!****"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the base model from the pre-trained model Resnet50\n#base_model = ResNet50(input_shape=IMG_SHAPE,\n                          #      include_top=False, \n                           #     weights='imagenet',\n                           #    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img_in = Input(IMG_SHAPE) \nbase_model = MobileNet(include_top= False , # remove  the 3 fully-connected layers at the top of the network\n                weights='imagenet', # pre train weight\n                input_shape= IMG_SHAPE\n                     ) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#base_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"base_model.trainable = False\nbase_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(IMG_SHAPE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel = tf.keras.Sequential([\n  base_model,\n  layers.GlobalAveragePooling2D(),\n  layers.Dense(NUM_CLASSES, activation='softmax', name=\"predictions\"),\n])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()\nplot_model(model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OPTIMIZER = Adam(lr=0.001)\nmodel.compile(optimizer=OPTIMIZER, \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True,expand_nested=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Find learning rate"},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to find/plot the learning rate using the LRFinder callback\n\ndef find_lr(MIN_LR=1e-5, MAX_LR=1e-2, STEPS=train_data.samples // BATCH_SIZE, \n            EPOCHS=3, CLASS_WEIGHTS={0: 1.0, 1: 3.3, 2: 4.3, 3: 1.4}, \n            beta = 0.99):\n    \n    lr_finder = LRFinder(min_lr=MIN_LR, \n                         max_lr=MAX_LR, \n                         steps_per_epoch=STEPS, \n                         epochs=EPOCHS,\n                         beta = beta)\n    \n    history = model.fit(\n        train_data,\n        steps_per_epoch=train_data.samples // BATCH_SIZE,\n        epochs=3,\n        #workers=3,\n        callbacks=[lr_finder],\n        class_weight=CLASS_WEIGHTS)\n    \n    lr_finder.plot_loss()\n    lr_finder.plot_lr()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"find_lr(CLASS_WEIGHTS=class_weights)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"A learning rate of 0.001 or 0.002 will give the best performance. The default for the Adam optimizer is 0.001 so we will use that value. This choice is supported by experiments we did using LR = 0.001, 0.002 and 0.01. As indicated by the LR curve above, 0.01 is probably too large and the model had problems converging. 0.002 was also suboptimal."},{"metadata":{},"cell_type":"markdown","source":"> **Feature Extraction**"},{"metadata":{"trusted":true},"cell_type":"code","source":"OPTIMIZER = Adam(lr=0.001)\nmodel.compile(optimizer=OPTIMIZER, \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_FE = model.fit(\n           train_data, \n           steps_per_epoch= train_data.samples // BATCH_SIZE, \n           epochs=FE_EPOCHS,\n           class_weight=class_weights, \n           #workers=4,\n           #validation_data=valid_data, \n           #validation_steps=valid_data.samples // BATCH_SIZE,\n           callbacks=[checkpoint])\n\nmodel.save_weights(FE_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot training curves\n    \naccuracy = history_FE.history['accuracy']\nloss = history_FE.history['loss']\ndisplay_training_curves(accuracy, loss, start_epoch=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Transfer Learning step 2: Fine-tuning****"},{"metadata":{},"cell_type":"markdown","source":"This is the second stage of transfer learning. We unfreeze some fraction of the layers and fit the model using smaller learning rate (Adam lr = 0.0001).\n\nWe address the data imbalance by:\n\nrebalancing the training dataset (oversampling with copies of DME and DRUSEN)\naugmenting the data\nusing class weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = get_train_set(rebalance=1)\ntrain_data = get_train_data(augmentation=1)\n    \nclass_weights = get_class_weights()\n    \nplot_dataset_distribution(df_train, df_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"    \nprint(\"Number of layers in the base model: \", len(base_model.layers))\n# plot_model(base_model)\n\n\nbase_model.trainable = True\n\n# freezing all layers except the batch normalization layers\n#for layer in base_model.layers:\n #   if isinstance(layer, BatchNormalization):\n  #     layer.trainable = True\n   # else:\n    #    layer.trainable = False \n\n#set_trainable = False\nfor layer in base_model.layers:\n    #if layer.name == 'block7a_se_excite':\n     #   set_trainable = True\n    #if set_trainable:\n    if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n    else:\n        layer.trainable = False\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"OPTIMIZER = Adam(lr=1e-4)\nmodel.compile(optimizer=OPTIMIZER, \n              loss='categorical_crossentropy', \n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history_FT_1 = model.fit(\n            train_data, \n            steps_per_epoch = train_data.samples // BATCH_SIZE, \n            epochs = FT_EPOCHS,\n            class_weight = class_weights, \n            #workers = 4,\n            #validation_data=valid_data, \n            #validation_steps=valid_data.samples // BATCH_SIZE,\n            callbacks=[checkpoint, lr_scheduler])\n    \nmodel.save_weights(FT_FILE)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The training dataset is now more balanced (but not completely equal)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot FE and FT learning curves \n\naccuracy += history_FT_1.history['accuracy']\nloss += history_FT_1.history['loss']\ndisplay_training_curves(accuracy, loss, start_epoch=FE_EPOCHS)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Test Performance****"},{"metadata":{"trusted":true},"cell_type":"code","source":"# for test resultes only\n# model.load_weights(MODEL_WEIGHTS_PATH+FT_FILE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_confusion_matrix(cm, \n                          classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Purples):\n   \n\n    accuracy = np.trace(cm) / float(np.sum(cm))\n    misclass = 1 - accuracy  \n    \n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    \n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, fontsize=15)\n    plt.yticks(tick_marks, classes, fontsize=15)\n    \n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'\n           .format(accuracy, misclass),fontsize=15)\n    plt.ylabel('True label', fontsize=15)\n    plt.title(title, fontsize=22);\n    plt.colorbar()\n\n    thresh = cm.max() / 2.0\n    for i, j in product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_test = get_test_set()\ntest_data = get_test_data()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_prob = model.predict(test_data, steps=len(test_data), verbose=1)\ny_pred = np.argmax(y_prob,axis=1)\ny_true = df_test['labels'].astype('int64').to_numpy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print results\n\nprint(classification_report(y_true, y_pred))\nprint('The accuracy is {}'.format(accuracy_score(y_true, y_pred)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# confusion matrix\n    \ncm = confusion_matrix(y_true,y_pred)\nclasses = np.array(CLASSES)\ntitle = 'Confusion matrix of results'\nplot_confusion_matrix(cm, classes, title)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FP = cm.sum(axis=0) - np.diag(cm)  \nFN = cm.sum(axis=1) - np.diag(cm)\nTP = np.diag(cm)\nTN = cm.sum() - (FP + FN + TP)\n\nFP = FP.astype(float)\nFN = FN.astype(float)\nTP = TP.astype(float)\nTN = TN.astype(float)\n\n# Sensitivity, hit rate, recall, or true positive rate\nTPR = TP/(TP+FN)\nprint(\"Recall/TPR = {}\".format(TPR))\n\n# Specificity or true negative rate\nTNR = TN/(TN+FP) \nprint(\"Specificity/TNR = {}\".format(TNR))\n\n# Precision or positive predictive value\nPPV = TP/(TP+FP)\nprint(\"Precision/PPV = {}\".format(PPV))\n\n# Negative predictive value\nNPV = TN/(TN+FN)\nprint(\"Negative Predict Value = {}\".format(NPV))\n\n# Fall out or false positive rate\nFPR = FP/(FP+TN)\nprint(\"False Positive Rate = {}\".format(FPR))\n\n# False negative rate\nFNR = FN/(TP+FN)\nprint(\"False Negative Rate = {}\".format(FNR))\n\n# False discovery rate\nFDR = FP/(TP+FP)\nprint(\"False discovery rate = {}\".format(FDR))\n\n# Overall accuracy\nACC = (TP+TN)/(TP+FP+FN+TN)\nprint(\"Overall Accuracy = {}\".format(ACC))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
